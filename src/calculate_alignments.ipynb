{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a3f9c2-6bfe-40ba-81ee-b76599d5bdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This remains here as a trophy of triumph over tribulation\n",
    "\"Â \" == \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29347a8b-f845-4098-b0ef-5f3457cdca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os.path\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "parentdir = os.getcwd()[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9876c9df-90fe-402d-a65b-b3b595ff43f8",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f012ca9-a0b5-4955-b47a-3624fbef0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersection_of_words(m1, m2, targetWord, k):\n",
    "    '''\n",
    "    description:\n",
    "        Finds top k words from m1 that are in m2\n",
    "    '''\n",
    "    # retrieve k*searchFactor nearest words as candidates\n",
    "    searchFactor = 150 \n",
    "    if targetWord == \"village\":\n",
    "        output = m1.wv.most_similar(targetWord, topn=k*3000) # village is especially sparse\n",
    "    else:\n",
    "        output = m1.wv.most_similar(targetWord, topn=k*searchFactor)\n",
    "    candidate_words = [word for word,_ in output]\n",
    "    final_words = list()\n",
    "    \n",
    "    for word in candidate_words:\n",
    "        try:\n",
    "            # breaks if word not in m2\n",
    "            m2.wv[word]\n",
    "            if len(final_words) < k:\n",
    "                final_words.append(word)\n",
    "            else:\n",
    "                break\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if len(final_words) == k:\n",
    "        return final_words\n",
    "    else:\n",
    "        sys.exit(f\"Neighborhood too sparse for word: {targetWord}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594bdab4-1466-4333-b4bd-95755e2f9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nearest_neighbors(m1, m2, targetWord, k):\n",
    "    '''\n",
    "    inputs:\n",
    "        m1: model the top k words will be derived from\n",
    "        m2: other model\n",
    "        targetWord: word from which top k words will be derived\n",
    "        k: number of words to compare\n",
    "        \n",
    "    output:\n",
    "        pearson correlation between the two datasets\n",
    "        difference in similarity between m1 and m2's average similarities to m1's k nearest neighbors\n",
    "        \n",
    "    description:\n",
    "        Returns the difference between the average similarity of the k most similar words to the \n",
    "        target word from m1 and the average similarity of the same words in m2.\n",
    "    '''\n",
    "    \n",
    "    # finds k most similar words in m1 that are in m2.\n",
    "    top_words = find_intersection_of_words(m1, m2, targetWord, k)\n",
    "\n",
    "    # stores similarities of top_words to target word\n",
    "    m1Similarities = list()\n",
    "    m2Similarities = list()\n",
    "    \n",
    "    m1TargetVec = m1.wv[targetWord]\n",
    "    m2TargetVec = m2.wv[targetWord]\n",
    "    # create list of similarities\n",
    "    for word in top_words:\n",
    "\n",
    "        m1ComparisonVec = m1.wv[word]\n",
    "        m1Similarity = 1 - spatial.distance.cosine(m1TargetVec, m1ComparisonVec)\n",
    "        m1Similarities.append(m1Similarity)\n",
    "        \n",
    "        m2ComparisonVec = m2.wv[word]\n",
    "        m2Similarity = 1 - spatial.distance.cosine(m2TargetVec, m2ComparisonVec)\n",
    "        m2Similarities.append(m2Similarity)\n",
    "        \n",
    "    # return (sum(m1Similarities) / len(m1Similarities)) - (sum(m2Similarities) / len(m2Similarities))\n",
    "    corr, p_value = pearsonr(m1Similarities, m2Similarities)\n",
    "    # if p_value > 0.1:\n",
    "    #     print(f\"p-value is: {p_value} for {targetWord}\")\n",
    "    return corr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef0199c7-f012-4a8c-835e-82d815ee39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_a_domain(m1, m2, wordList, k):\n",
    "    '''\n",
    "    description:\n",
    "        Calculates the average pearson correlation values of words in a domain as well as the standard deviation.\n",
    "        \n",
    "        NOTE: Function is for testing/debugging purposes only.\n",
    "    '''\n",
    "    similarities = list()\n",
    "    for word in wordList:\n",
    "        wordComparison = (compare_nearest_neighbors(m1, m2, word, k) + compare_nearest_neighbors(m2, m1, word, k)) / 2\n",
    "        similarities.append(wordComparison)\n",
    "\n",
    "    data = pd.Series(similarities)\n",
    "     \n",
    "    return  f\"Mean difference: {round(data.mean(), 4)}     Standard Deviation: {round(data.std(), 4)}\"\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a50323-972b-4d1b-ad36-2f130195134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_domains(m1, m2, k, domains_dict):\n",
    "    '''\n",
    "    description:\n",
    "        Calculates the average and standard deviation of alignment of words across domains.\n",
    "    input:\n",
    "        m1: one of two models. Order doesn't matter\n",
    "        m2: second of two models. Order doesn't matter\n",
    "        k: how many nearest words to compare when calculating alignment of single word\n",
    "        domains_dict: keys are domain names, values are lists of words of that domain\n",
    "    output:\n",
    "        dataframe containing alignment statistics across domains\n",
    "    '''\n",
    "    results = {\"Domain\": [], \"Alignment\": [], \"Standard Deviation\": []}\n",
    "    domains = list(domains_dict.keys())\n",
    "    alignments = []\n",
    "    std = []\n",
    "    granular_results = dict()\n",
    "    for domain in domains:\n",
    "        wordList = domains_dict[domain]\n",
    "        similarities = list()\n",
    "        for word in wordList:\n",
    "            wordComparison = (compare_nearest_neighbors(m1, m2, word, k) + compare_nearest_neighbors(m2, m1, word, k)) / 2\n",
    "            similarities.append(wordComparison)\n",
    "        \n",
    "        # get all domain values\n",
    "        granular_results.update({domain:similarities})\n",
    "\n",
    "        # get stats on domain values\n",
    "        similarity_stats = pd.Series(similarities)\n",
    "        alignments.append(round(similarity_stats.mean(), 4))\n",
    "        std.append(round(similarity_stats.std(), 4))\n",
    "        \n",
    "        \n",
    "    return granular_results, pd.DataFrame({\"Domain\" : domains, \"Alignments\": alignments, \"Standard Deviation\": std})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c592d-5cc8-46cf-b84c-aa4e1106e54d",
   "metadata": {},
   "source": [
    "### Load and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd75d1f-c646-464a-ac1c-c2d8f1ad6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domains.json includes all domain words that exist in both datasets\n",
    "with open(os.path.join(parentdir, \"data\", \"domains.json\"), \"rt\") as f:\n",
    "    domains = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbc915a-2898-422a-a67c-43523ce6cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec.load(os.path.join(parentdir, \"models\", \"NANTeC_clean.bin\"))\n",
    "model2 = Word2Vec.load(os.path.join(parentdir, \"models\", \"indicorp_clean.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df5ba8a5-d2ad-4e64-a5fd-b6fca8815989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = how many nearest words to compare when calculating alignment of single word\n",
    "k = 100\n",
    "# run code\n",
    "granular_results, stat_results = compare_all_domains(model1, model2, k, domains)\n",
    "stat_results = stat_results.sort_values(by=['Alignments'], ascending=False, ignore_index=True)\n",
    "# write code to results\n",
    "with open(os.path.join(parentdir, \"results\", f\"results_statistical_{k}.csv\"), \"wt\") as f:\n",
    "    stat_results.to_csv(f)\n",
    "# load and write granular results to csv\n",
    "granular_results_df = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in granular_results.items()]))\n",
    "with open(os.path.join(parentdir, \"results\", f\"results_full_{k}.csv\"), \"wt\") as f:\n",
    "    granular_results_df.to_csv(f)\n",
    "# display results\n",
    "granular_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08410559-b8b0-4717-9f89-3287de23d198",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b540ae1-953f-41a3-bd65-25903078ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nearest_neighbors2(m1, m2, targetWord, k):\n",
    "    '''\n",
    "    description:\n",
    "        Used to facilitate making the alignments figure.\n",
    "    '''\n",
    "    \n",
    "    # finds k most similar words in m1 that are in m2.\n",
    "    top_words = find_intersection_of_words(m1, m2, targetWord, k)\n",
    "\n",
    "    # stores similarities of top_words to target word\n",
    "    m1Similarities = list()\n",
    "    m2Similarities = list()\n",
    "    # comparisons = list()\n",
    "    \n",
    "    m1TargetVec = m1.wv[targetWord]\n",
    "    m2TargetVec = m2.wv[targetWord]\n",
    "    # create list of similarities\n",
    "    for word in top_words:\n",
    "\n",
    "        m1ComparisonVec = m1.wv[word]\n",
    "        m1Similarity = round(1 - spatial.distance.cosine(m1TargetVec, m1ComparisonVec), 3)\n",
    "        m1Similarities.append([word, m1Similarity])\n",
    "        \n",
    "        m2ComparisonVec = m2.wv[word]\n",
    "        m2Similarity = round(1 - spatial.distance.cosine(m2TargetVec, m2ComparisonVec), 3)\n",
    "        m1Similarities[-1].append(m2Similarity)\n",
    "        m1Similarities[-1].append(round((m1Similarities[-1][2] - m1Similarities[-1][1]), 2))\n",
    "        \n",
    "        # comparisons.append(round((m1Similarity - m2Similarity), 3))\n",
    "        \n",
    "        \n",
    "\n",
    "    return m1Similarities\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6022f69-969e-49e8-bdca-cbcefcb920cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sweltering', 0.674, 0.509, -0.17],\n",
       " ['humid', 0.659, 0.466, -0.19],\n",
       " ['scorching', 0.653, 0.358, -0.3],\n",
       " ['broiling', 0.642, 0.345, -0.3],\n",
       " ['humidity', 0.634, 0.546, -0.09],\n",
       " ['dust', 0.621, 0.461, -0.16],\n",
       " ['vapors', 0.614, 0.398, -0.22],\n",
       " ['heatwave', 0.612, 0.334, -0.28],\n",
       " ['cooler', 0.598, 0.421, -0.18],\n",
       " ['heating', 0.596, 0.455, -0.14],\n",
       " ['convection', 0.588, 0.438, -0.15]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_nearest_neighbors2(model2, model1, \"heat\", 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
